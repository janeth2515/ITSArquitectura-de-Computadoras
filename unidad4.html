<!DOCTYPE html>
<html lang="es">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Arquitectura de computadoras</title>

    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1"
      crossorigin="anonymous"
    />

    <!-- import the webpage's stylesheet -->
    <link rel="stylesheet" href="/style.css" />
  </head>
  <body>
    <header>
      <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <ul class="nav">
          <li class="nav-item">
            <a class="nav-link active" aria-current="page" href="index.html"
              >Inicio</a
            >
          </li>
          <li class="nav-item">
            <a class="nav-link" href="unidad1.html">Unidad 1</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="unidad2.html">Unidad 2</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="unidad3.html">Unidad 3</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="unidad4.html">Unidad 4</a>
          </li>
        </ul>
      </nav>
      <div
        class="cover d-flex justify-content-center align-items-center flex-column"
      >
        <h1>
          Arquitectura de computadoras
        </h1>
        <p>
          Unidad 4 Procesamiento Paralelo.
        </p>
      </div>
    </header>
  </body>
  <section>
    <div class="conteiner mt-5 nt-5">
      <h2 class="title">
        <p style="color:#3C75C8">
          4.1 Aspectos básicos de la computación paralela.
        </p>
      </h2>
      <p>
        Computador paralelo: Conjunto de elementos de procesos independientes
        que operan de una forma conjunta para resolver problemas de elevado
        coste computacional.La computación paralela es una forma de cómputo en
        la que muchas instrucciones se ejecutan simultáneamente, operando sobre
        el principio de que problemas grandes, a menudo se pueden dividir en
        unos más pequeños, que luego son resueltos simultáneamente (en
        paralelo). Hay varias formas diferentes de computación paralela:
        paralelismo a nivel de bit, paralelismo a nivel de instrucción,
        paralelismo de datos y paralelismo de tareas. El paralelismo se ha
        empleado durante muchos años, sobre todo en la computación de altas
        prestaciones, pero el interés en ella ha crecido últimamente debido a
        las limitaciones físicas que impiden el aumento de la frecuencia. Como
        el consumo de energía —y por consiguiente la generación de calor— de las
        computadoras constituye una preocupación en los últimos años, la
        computación en paralelo se ha convertido en el paradigma dominante en la
        arquitectura de computadores, principalmente en forma de procesadores
        multinúcleo.
      </p>
    </div>
    <div class="conteiner mt-5 nt-5">
      <p>
        Las computadoras paralelas pueden clasificarse según el nivel de
        paralelismo que admite su hardware: equipos con procesadores multinúcleo
        y multi-procesador que tienen múltiples elementos de procesamiento
        dentro de una sola máquina y los clústeres, MPPS y grids que utilizan
        varios equipos para trabajar en la misma tarea. Muchas veces, para
        acelerar la tareas específicas, se utilizan arquitecturas especializadas
        de computación en paralelo junto a procesadores tradicionales. Los
        programas informáticos paralelos son más difíciles de escribir que los
        secuenciales, porque la concurrencia introduce nuevos tipos de errores
        de software, siendo las condiciones de carrera los más comunes. La
        comunicación y sincronización entre diferentes subtareas son algunos de
        los mayores obstáculos para obtener un buen rendimiento del programa
        paralelo. La máxima aceleración posible de un programa como resultado de
        la paralelización se conoce como la ley de Amdahl.
      </p>
    </div>
  </section>
  <section>
    <div class="conteiner mt-5 nt-5">
      <h2 class="title">
        <p style="color:#3C75C8">
          4.2 Tipos de computación paralela.
        </p>
      </h2>
      <p>
        Paralelismo a nivel de bit: Desde el advenimiento de la integración a
        gran escala (VLSI) como tecnología de fabricación de chips de
        computadora en la década de 1970 hasta alrededor de 1986, la aceleración
        en la arquitectura de computadores se lograba en gran medida duplicando
        el tamaño de la palabra en la computadora, la cantidad de información
        que el procesador puede manejar por ciclo. El aumento del tamaño de la
        palabra reduce el número de instrucciones que el procesador debe
        ejecutar para realizar una operación en variables cuyos tamaños son
        mayores que la longitud de la palabra. Por ejemplo, cuando un procesador
        de 8 bits debe sumar dos enteros de 16 bits, el procesador primero debe
        adicionar los 8 bits de orden inferior de cada número entero con la
        instrucción de adición, a continuación, añadir los 8 bits de orden
        superior utilizando la instrucción de adición con acarreo que tiene en
        cuenta el bit de acarreo de la adición de orden inferior, en este caso
        un procesador de 8 bits requiere dos instrucciones para completar una
        sola operación, en donde un procesador de 16 bits necesita una sola
        instrucción para poder completarla. Históricamente, los
        microprocesadores de 4 bits fueron sustituidos por unos de 8 bits, luego
        de 16 bits y 32 bits, esta tendencia general llegó a su fin con la
        introducción de procesadores de 64 bits, lo que ha sido un estándar en
        la computación de propósito general durante la última década.
      </p>
    </div>
    <div class="conteiner mt-5 nt-5">
      <h2 class="title">
        <p style="color:#3C75C8">
          4.2.1 Taxonomía de las arquitecturas paralelas.
        </p>
      </h2>
      <p>
        La clasificación de Flynn ha demostrado funcionar bastante bien para la
        tipificación de sistemas, y se ha venido usando desde décadas por la
        mayoría de los arquitectos de computadores. Sin embargo, los avances en
        tecnología y diferentes topologías, han llevado a sistemas que no son
        tan fáciles de clasificar dentro de los 4 tipos de Flynn. Por ejemplo,
        los procesadores vectoriales no encajan adecuadamente en esta
        clasificación, ni tampoco las arquitecturas hibridas. Para solucionar
        esto se han propuesto otras clasificaciones, donde los tipos SIMD y MIMD
        de Flynn se suelen conservar, pero que sin duda no han tenido el éxito
        de la de Flynn.
      </p>
    </div>

    <div class="conteiner mt-5 nt-5">
      <h2 class="title">
        <p style="color:#3C75C8">
          4.3 Sistemas de memoria compartida: multiprocesadores.
        </p>
      </h2>
      <p>
        Un multiprocesador puede verse como un computador paralelo compuesto por
        varios procesadores interconectados que comparten un mismo sistema de
        memoria. Los sistemas multiprocesadores son arquitecturas MIMD con
        memoria compartida. Tienen un único espacio de direcciones para todos
        los procesadores y los mecanismos de comunicación se basan en el paso de
        mensajes desde el punto de vista del programador. Dado que los
        multiprocesadores comparten diferentes módulos de memoria, pudiendo
        acceder a un mismo módulo varios procesadores, a los multiprocesadores
        también se les llama sistemas de memoria compartida.
      </p>
    </div>
    <div class="conteiner mt-5 nt-5">
      <p>
        Dependiendo de la forma en que los procesadores comparten la memoria, se
        clasifican en sistemas multiprocesador UMA, NUMA y COMA. Multiproceso es
        tradicionalmente conocido como el uso de múltiples procesos concurrentes
        en un sistema en lugar de un único proceso en un instante determinado.
        Como la multitarea que permite a múltiples procesos compartir una única
        CPU, múltiples CPUs pueden ser utilizados para ejecutar múltiples hilos
        dentro de un único proceso. El multiproceso para tareas generales es, a
        menudo, bastante difícil de conseguir debido a que puede haber varios
        programas manejando datos internos (conocido como estado o contexto) a
        la vez. Los programas típicamente se escriben asumiendo que sus datos
        son incorruptibles. Sin embargo, si otra copia del programa se ejecuta
        en otro procesador, las dos copias pueden interferir entre sí intentando
        ambas leer o escribir su estado al mismo tiempo. Para evitar este
        problema se usa una variedad de técnicas de programación incluyendo
        semáforos y otras comprobaciones y bloqueos que permiten a una sola
        copia del programa cambiar de forma exclusiva ciertos valores.
      </p>
    </div>

    <div class="conteiner mt-5 nt-5">
      <h2 class="title">
        <p style="color:#3C75C8">
          4.4 Sistemas de memoria distribuida. Multicomputadoras Clusters.
        </p>
      </h2>
      <p>
        Los sistemas de memoria distribuida o multicomputadores pueden ser de
        dos tipos básicos. El primer de ellos consta de un único computador con
        múltiples CPUs comunicadas por un bus de datos mientras que en el
        segundo se utilizan múltiples computadores, cada uno con su propio
        procesador, enlazados por una red de interconexión más o menos rápida.
        Sobre los sistemas de multicomputadores de memoria distribuida, se
        simula memorias compartidas. Se usan los mecanismos de comunicación y
        sincronización de sistemas multiprocesadores. Un clúster es un tipo de
        arquitectura paralela distribuida que consiste de un conjunto de
        computadores independientes interconectados operando de forma conjunta
        como único recurso computacional sin embargo, cada computador puede
        utilizarse de forma independiente o separada. En esta arquitectura, el
        computador paralelo es esencialmente una colección de procesadores
        secuenciales, cada uno con su propia memoria local, que pueden trabajar
        conjuntamente.
      </p>
      <li class="list-group-item">
        Cada nodo tiene rápido acceso a su propia memoria y acceso a la memoria
        de otros nodos mediante una red de comunicaciones, habitualmente una red
        de comunicaciones de alta velocidad.
      </li>
      <li class="list-group-item">
        Los datos son intercambiados entre los nodos como mensajes a través de
        la red.
      </li>
      <li class="list-group-item">
        Una red de ordenadores, especialmente si disponen de una interconexión
        de alta velocidad, puede ser vista como un multicomputador de memoria
        distribuida y como tal ser utilizada para resolver problemas mediante
        computación paralela.
      </li>
    </div>
    <div align= "center">
      
      <img src="https://cdn.glitch.com/4c8cc7d1-9617-4086-a6d3-711095e7111d%2Fcbusqueda.jpg?v=1610724973031">
    </div>
    <div class="conteiner mt-5 nt-5">
      <h2 class="title">
        <p style="color:#3C75C8">
          4.5 Casos de estudio.
        </p>
      </h2>
      <p>
        Por numerosos motivos, el procesamiento distribuido se ha convertido en
        un área de gran importancia e interés dentro de la Ciencia de la
        Computación, produciendo profundas transformaciones en las líneas de
        I/D. Interesa realizar investigación en la especificación,
        transformación, optimización y evaluación de algoritmos distribuidos y
        paralelos. Esto incluye el diseño y desarrollo de sistemas paralelos, la
        transformación de algoritmos secuenciales en paralelos, y las métricas
        de evaluación de performance sobre distintas plataformas de soporte
        (hardware y software). Más allá de las mejoras constantes en las
        arquitecturas físicas de soporte, uno de los mayores desafíos se centra
        en cómo aprovechar al máximo la potencia de las mismas. Interesa
        realizar investigación en la especificación, transformación,
        optimización y evaluación de algoritmos distribuidos y paralelos. Esto
        incluye el diseño y desarrollo de sistemas paralelos, la transformación
        de algoritmos secuenciales en paralelos, y las métricas de evaluación de
        performance sobre distintas plataformas de soporte (hardware y
        software). Más allá de las mejoras constantes en las arquitecturas
        físicas de soporte, uno de los mayores desafíos se centra en cómo
        aprovechar al máximo la potencia de las mismas.
      </p>
    </div>
  </section>
  <section>
    <div class="conteiner mt-5 nt-5" text-align="center">
      <h2 class="title">
        <p style="color:#3C75C8">Procesamiento paralelo</p>
      </h2>
    </div>
    <div align ="center">
      
    
    <iframe
      src="https://docs.google.com/presentation/d/e/2PACX-1vQ4o-OtiDOBfZ_R2SiKbswUYbI7fL3GOUbrNmdpe9D20GpRbUZXLBfmephhJpSV7fMeQAnkX89yVMa_/embed?start=false&loop=false&delayms=3000"
      frameborder="50"
      width="500"
      height="230"
      allowfullscreen="true"
      mozallowfullscreen="true"
      webkitallowfullscreen="true"
    ></iframe>
      </div>
  </section>
</html>
